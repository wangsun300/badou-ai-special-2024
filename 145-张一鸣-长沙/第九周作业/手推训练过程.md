## 神经网络训练过程

1.定义神经网络结构（隐藏层数、激活函数等等）

2.提取特征向量样本作为输入，并进行标准化/归一化处理，如选择样本集合的一个样本 (Ai, Bi)，Ai 为样本数据，Bi 为标签

3.初始权重为随机值，将数据输入神经网络进行前向传播，计算激活函数的实际输出 y

4.计算损失函数 D=Bi-y，即实际输出 y 与标签期望值 Bi 的差异

5.根据误差 D 的大小，可利用梯度下降法计算偏导，反向传播优化更新权重矩阵 w

6.重复上述3~5过程，直到误差 D 达到限定范围的停止条件



## 数据标准化常用方法

### 1.归一化

将数据统一映射到[0,1]区间。

​	y = (x-min)/(max-min)

### 2.零均值化

经处理后数据均值为0，标准差为1（正态分布），μ为样本均值，σ为样本标准差。

​	y = (x-μ)/σ



## 损失函数常用算法

### 1.均值平方差/均方误差（MSE）

适合输入的标签为实数、无界的值时使用。
$$
MSE = \sum_{i=1}^n \frac{1}n (f(x \tiny i \normalsize) - y \tiny i \normalsize)^2
$$

### 2.交叉熵

适合输入的标签为位矢量（分类标志）时使用。一般用于分类问题，表达意思为预测输入样本属于哪一类的概率，值越小，代表预测结果越准。（y 代表真实值分类（0或1），a 代表预测值）
$$
C = - \frac{1}n \sum_{x} [yln \tiny a \normalsize +(1-y)ln(1-a)]
$$


## 过拟合的解决办法

### 1.Early Stopping

### 2.Dropout

### 3.减少特征

### 4.更多的输入训练样本

### 5.重新清洗数据



## 手推训练过程

以第一层输入层（包含两个神经元i1、i2和偏置b1）

第二层隐藏层（包含两个神经元h1,、h2和偏置b2）

第三层输出层（包含o1、o2）的神经网络结构为例

wi为层与层之间连接的权重，学习率 η=0.5

激活函数选用sigmod函数

以 z 表示某神经元的加权输入和，a 表示某神经元的输出

<img src=".\神经网络训练.png" alt="image-20240620133857924" style="zoom:50%;" />



### 1.前向传播过程

#### ● 输入层→隐藏层

计算隐藏层神经元 h1 的加权输入和 zh1：
$$
z \tiny h1 \normalsize = w \tiny 1  \normalsize * i \tiny 1  \normalsize + w \tiny 2  \normalsize * i \tiny 2  \normalsize + b \tiny 1 \\
\normalsize  = 0.15 * 0.05 + 0.2 * 0.1 + 0.35
\\= 0.3775
$$
计算过激活函数后 h1 的输出值 ah1：
$$
a \tiny h1 = \frac{1}{1+e^{-z \tiny h1}}\\
=\frac{1}{1+e^{-0.3775}}\\
=0.593269992
$$
同样计算出 h2 的输出 ah2：
$$
z\tiny h2 \normalsize = w \tiny 3 \normalsize * i \tiny 1 \normalsize + w \tiny 4 \normalsize * i \tiny 2 \normalsize + b \tiny 1\\
= 0.25 * 0.05 + 0.3 * 0.1 + 0.35\\
=0.3925\\
\\
a\tiny h2 \normalsize = \frac{1}{1+e^{-z\tiny h2}}\\
=\frac{1}{1+e^{-0.3925}}\\
=\frac{1}{1+0.6742}\\
≈0.5957
$$


#### ● 隐藏层→输出层

同样计算出输出层 ao1、ao2：
$$
z\tiny o1 \normalsize = w \tiny 5 \normalsize * h \tiny 1 \normalsize + w \tiny 6 \normalsize * h \tiny 2 \normalsize + b \tiny 2\\
= 0.4 * 0.5932 + 0.45 * 0.5957 + 0.6\\
≈1.1053\\
\\
a\tiny o1 \normalsize = \frac{1}{1+e^{-z\tiny o1}}\\
=\frac{1}{1+e^{-1.1053}}\\
=\frac{1}{1+0.3321}\\
≈0.7519
$$

$$
z\tiny o2 \normalsize = w \tiny 7 \normalsize * h \tiny 1 \normalsize + w \tiny 8 \normalsize * h \tiny 2 \normalsize + b \tiny 2\\
= 0.5 * 0.5932 + 0.55 * 0.5957 + 0.6\\
≈1.2242\\
\\
a\tiny o2 \normalsize = \frac{1}{1+e^{-z\tiny o2}}\\
=\frac{1}{1+e^{-1.2242}}\\
=\frac{1}{1+0.2955}\\
≈0.7716
$$

综上，实际输出 [0.7519, 0.7716] 与期望值 [0.01, 0.99] 差距较大，需要继续下一步优化权重。

### 2.反向传播过程

#### ● 使用MSE，分别计算输出 o1、o2 的损失函数，总误差为两者之和

$$
E \tiny o1 \normalsize = \sum\frac{1}2(target - outut)^2\\
=\sum\frac{1}2(0.01 - 0.7519)^2\\
=0.2752\\
\\
E \tiny o2 \normalsize = \sum\frac{1}2(target - outut)^2\\
=\sum\frac{1}2(0.99 - 0.7716)^2\\
=0.0238\\
\\
E = E \tiny o1 \normalsize + E \tiny o2\\
=0.2752 + 0.0238\\
=0.299
$$



#### ● 隐藏层→输出层的权重更新

用整体损失 E 对隐藏层各权重参数求偏导：
$$
\frac{\partial E}{\partial w \tiny 5} =\frac{\partial E}{\partial a \tiny o1} * \frac{\partial a \tiny o1}{\partial z \tiny o1} * \frac{\partial z \tiny o1}{\partial w \tiny 5}\\
=0.7414 * 0.1868 * 0.5933\\
=0.0821\\
\\
\frac{\partial E}{\partial w \tiny 6} =\frac{\partial E}{\partial a \tiny o1} * \frac{\partial a \tiny o1}{\partial z \tiny o1} * \frac{\partial z \tiny o1}{\partial w \tiny 6}\\
=0.4087\\
\\
\frac{\partial E}{\partial w \tiny 7} =\frac{\partial E}{\partial a \tiny o2} * \frac{\partial a \tiny o2}{\partial z \tiny o2} * \frac{\partial z \tiny o2}{\partial w \tiny 7}\\
=0.5113\\
\\
\frac{\partial E}{\partial w \tiny 8} =\frac{\partial E}{\partial a \tiny o1} * \frac{\partial a \tiny o1}{\partial z \tiny o1} * \frac{\partial z \tiny o1}{\partial w \tiny 8}\\
=0.5614
$$
然后更新权重 wi，新权重 = 原权重 - 学习率 * 梯度：
$$
w_5^+ = w_5 -\eta * \frac{\partial E}{\partial w_5}\\
=0.4-0.5*0.0821\\
=0.3589\\
\\
w_6^+ = w_6 -\eta * \frac{\partial E}{\partial w_6}\\
=0.4087\\
\\
w_7^+ = w_7 -\eta * \frac{\partial E}{\partial w_7}\\
=0.5113\\
\\
w_8^+ = w_8 -\eta * \frac{\partial E}{\partial w_8}\\
=0.5614\\
\\
$$


#### ● 输入层→隐藏层的权重更新

更新隐藏层权重后，继续计算总损失与输入层权重偏导，反向更新输入层权重：
$$
\frac{\partial E}{\partial w \tiny 1} =\frac{\partial E}{\partial a \tiny h1} * \frac{\partial a \tiny h1}{\partial z \tiny h1} * \frac{\partial z \tiny h1}{\partial w \tiny 1}\\
=(\frac{\partial E \tiny o1}{\partial a \tiny h1}+\frac{\partial E \tiny o2}{\partial a \tiny h1})* \frac{\partial a \tiny h1}{\partial z \tiny h1} * \frac{\partial z \tiny h1}{\partial w \tiny 1}\\
=0.0004\\
\\
\frac{\partial E}{\partial w \tiny 2} =\frac{\partial E}{\partial a \tiny h1} * \frac{\partial a \tiny h1}{\partial z \tiny h1} * \frac{\partial z \tiny h1}{\partial w \tiny 1}\\
=(\frac{\partial E \tiny o1}{\partial a \tiny h1}+\frac{\partial E \tiny o2}{\partial a \tiny h1})* \frac{\partial a \tiny h1}{\partial z \tiny h1} * \frac{\partial z \tiny h1}{\partial w \tiny 2}\\
\\
\frac{\partial E}{\partial w \tiny 3} =\frac{\partial E}{\partial a \tiny h2} * \frac{\partial a \tiny h2}{\partial z \tiny h2} * \frac{\partial z \tiny h2}{\partial w \tiny 1}\\
=(\frac{\partial E \tiny o1}{\partial a \tiny h2}+\frac{\partial E \tiny o2}{\partial a \tiny h2})* \frac{\partial a \tiny h2}{\partial z \tiny h2} * \frac{\partial z \tiny h2}{\partial w \tiny 3}\\
\\
\frac{\partial E}{\partial w \tiny 4} =\frac{\partial E}{\partial a \tiny h2} * \frac{\partial a \tiny h2}{\partial z \tiny h2} * \frac{\partial z \tiny h2}{\partial w \tiny 1}\\
=(\frac{\partial E \tiny o1}{\partial a \tiny h2}+\frac{\partial E \tiny o2}{\partial a \tiny h2})* \frac{\partial a \tiny h2}{\partial z \tiny h2} * \frac{\partial z \tiny h2}{\partial w \tiny 4}\\
\\
$$

$$
w_1^+ = w_1 -\eta * \frac{\partial E}{\partial w_1}\\
=0.1498\\
\\
w_2^+ = w_2 -\eta * \frac{\partial E}{\partial w_2}\\
=0.1996\\
\\
w_3^+ = w_3 -\eta * \frac{\partial E}{\partial w_3}\\
=0.2498\\
\\
w_4^+ = w_4 -\eta * \frac{\partial E}{\partial w_4}\\
=0.2995\\
\\
$$

至此完成一轮迭代，根据总误差大小判断是否停止迭代，训练得到最优模型可进行推理验证。