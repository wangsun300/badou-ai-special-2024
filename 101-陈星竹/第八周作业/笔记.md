### 线性回归--最小二乘法

**只适合误差较小的情况**

* 通过最小化误差的平方和，寻找数据最佳函数匹配

  ```
  数据点集（xi,yi）(i=1,2,3,....,m),给出拟合函数h(x)
  对于xi对应的估计值为h(xi)
  那么误差/残差就是： ri = h(xi)-yi
  
  ```

* 三种范数
  * 无穷大-范数：残差绝对值的最大值， max|ri| (1<=i<=m)
  * 1-范数: 绝对残差和，所有点的残差距离之和，|r1+r2+...+rm|
  * 2-范数：残差平方和 r1^2+r2^2+....+rm^2

* 拟合程度：拟合函数h(x）和待求解的函数y之间的相似性，2-范数越小，自然相似性越高。

#### 线性回归的参数求解

在线性回归中，我们需要求解参数 \(k\) 和 \(b\) 使得目标函数最小化。

对 \(k\) 和 \(b\) 求偏导并设为零，我们得到以下两个方程：

$$
\sum_{n=1}^{N} x_n y_n = k \sum_{n=1}^{N} x_n^2 + b \sum_{n=1}^{N} x_n
$$

$$
\sum_{n=1}^{N} y_n = k \sum_{n=1}^{N} x_n + bN
$$

通过解这两个方程，可以得到 \(k\) 和 \(b\) 的解：

$$
k = \frac{N \sum_{n=1}^{N} x_n^2 - \left(\sum_{n=1}^{N} x_n\right)^2}{N \sum_{n=1}^{N} x_n y_n - \sum_{n=1}^{N} x_n \sum_{n=1}^{N} y_n}
$$

$$
b = \frac{\sum_{n=1}^{N} y_n - k \sum_{n=1}^{N} x_n}{N}
$$

因此，我们得到了线性回归的最优参数 \(k\) 和 \(b\)。

````python
#最小二乘法
class LinearLeastSquareModel():
    def __init__(self,input_columns,output_columns):
        self.input_columns = input_columns
        self.output_columns = output_columns

    def fit(self,data):
        # np.vstack按垂直方向（行顺序）堆叠数组构成一个新的数组\
        input = np.vstack([data[:,i] for i in self.input_columns]).T
        output = np.vstack([data[:,i] for i in self.output_columns]).T
        '''
        x, resids, rank, s = sl.lstsq(A, B): 使用 SciPy 的 lstsq 方法进行最小二乘拟合，
        返回拟合参数 x，残差 resids，矩阵的秩 rank 和奇异值 s。
        '''
        x,resids,rank,s = sl.lstsq(input,output)
        #返回参数
        return x

    def get_err(self,data,model):
        #取出数据并转换成行
        A = np.vstack([data[:, i] for i in self.input_columns]).T  # 第一列Xi-->行Xi
        B = np.vstack([data[:, i] for i in self.output_columns]).T  # 第二列Yi-->行Yi
        B_fit = np.dot(A,model)
        err_point = np.sum((B-B_fit)**2,axis=1) #矩阵相减，差值平方，axis=1表示沿着行求和

        return err_point
````



### RANSAC——随机采样一致性

RANSAC是一种迭代方法，用于估计数学模型中的参数，特别是在数据中存在大量异常值（outliers）的情况下。RANSAC的主要思想是通过随机抽样和一致性判断来区分内点和外点，从而拟合出可靠的模型。

#### 原理

* RANSAC的**输入**：
  * 一组观测数据（含有较大的噪声或者无效点）
  * 一个用于解释观测数据的参数化模型（曲线/直线y=kx+b）
  * 一些可信的参数
* RANSAC的**输出**：通过迭代计算得到的参数化模型的参数（k,b）

* RANSAC算法的**步骤**如下：

  1. **随机采样**： 从数据集中随机选择最小数量的样本点来拟合模型。所需的最小样本点数量取决于所拟合模型的类型。例如，对于2D直线拟合，需要2个点。

  2. **模型拟合**： 用选定的样本点拟合模型（通过最小二乘法得到的一些k,b参数）（如直线、平面等）。

  3. **计算一致性**： 计算所有数据点与拟合模型的距离，判断哪些点与模型的拟合程度良好，即这些点是否为内点。通常通过设置一个阈值来决定是否为内点。如果数据点与模型的距离小于阈值，则认为该点是内点。
  4. **记录内点数量**： 记录当前模型的内点数量和模型参数。如果当前模型的内点数量大于之前记录的最大内点数量，则更新最大内点数量和模型参数。
  5. **迭代**： 重复步骤1到4，进行预定的迭代次数或直到找到足够好的模型。
  6. **模型优化**： 用所有的内点重新拟合模型，以得到更精确的参数估 计。

* RANSAC的参数：
  * n：一开始要随机选择多少点
  * k：要重复迭代多少次

```python
def ransac(data,model,n,t,d,k,debug=False ):
    '''
    输入：
        data：数据，model：拟合模型，n：生成模型需要的最小样本点，t:误差阈值，d:需要样本点的最小个数，k:最大迭代次数
    输出：
        bestfit:拟合参数
    '''
    besterr = t
    bestfit = None
    iterations = 0 #当前迭代次数

    while iterations < k:
        #候选模型样本点 验证候选模型点
        maybe_idx,test_idx = random_partition(n,data.shape[0])
        #取点
        maybe_liners = data[maybe_idx,:]
        test_points = data[test_idx,:]
        #拟合模型
        maybefit = model.fit(maybe_liners)
        #验证其他点的误差
        test_err = model.get_err(test_points, maybefit)
        #取出测试集中误差小于阈值t的点的索引
        #test_err < t 生成一个布尔数组，表示哪些位置的误差小于 t。
        #布尔索引是一种使用布尔数组（布尔值 True 和 False）来选择 NumPy 数组中元素的方法。当布尔数组被用于索引另一个数组时，只有布尔值为 True 的位置会被选中。
        also_idx = test_idx[test_err < t]
        also_liners = data[also_idx,:] #同样拟合模型的数据

        if debug:
            print ('test_err.min()',test_err.min())
            print ('test_err.max()',test_err.max())
            print ('numpy.mean(test_err)',np.mean(test_err))
            print ('iteration %d:len(alsoinliers) = %d' %(iterations, len(also_liners)) )
        #如果点数量大于阈值d，合并到测试集中
        if(len(also_liners)>d):
            betterdata = np.concatenate((maybe_liners,also_liners))
            betterfit = model.fit(betterdata)
            better_err = model.get_err(betterdata,betterfit)
            #取平均误差为当前误差
            this_err = np.mean(better_err)
            if(this_err<besterr):
                besterr = this_err
                bestfit = betterfit
        #增加迭代次数
        iterations = iterations + 1

    if bestfit is None:
        #没有找到匹配的拟合参数,抛出异常
        raise ValueError("did't meet fit acceptance criteria")
    else:
        return bestfit
```



## 相似图像搜索的哈希算法

哈希算法是一个函数，能够把几乎所有数字文件都转换成一串由数字和字母构成的看似乱码的字符串，即为哈希值，哈希值是一段数据唯一且机器紧凑的数值表示形式。

#### 特点

* 不可逆性
* 输出值唯一性
* 不可预测性

#### 汉明距离

​	两个整数之间的汉明距离指的是这两个数字对应二进制位不同的位置的数目。

### 均值哈希算法

#### 步骤

1. 将图像缩放到一个固定的大小，例如8x8像素。
2. 将缩放后的图像转换为灰度图像。
3. 计算图像所有像素的平均灰度值。
4. 根据每个像素的灰度值与平均灰度值的比较，大于平均值记为1，小于记为0，生成一个二进制哈希码。
5. 对比二进制哈希码，计算汉明距离，距离越小，差别越小

这种哈希算法的优点是简单易实现，并且对图像的缩放、旋转等变换具有一定的鲁棒性。然而，它也有一些缺点，例如对于一些复杂的图像，可能无法很好地捕捉到图像的特征。

```pyth
#均值哈希
def aHash(img):
    img = cv2.resize(img,(8,8))
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    mean = np.mean(gray)
    hashstr = ''
    for i in range(gray.shape[0]):
        for j in range(gray.shape[1]):
            if gray[i][j] > mean:
                hashstr += 1
            else:
                hashstr += 0
    return hashstr

```



### 差值哈希算法

#### 步骤

1. 将图像缩放到一个固定的大小，例如8x9像素。
2. 将缩放后的图像转换为灰度图像。
3. 计算相邻像素之间的灰度差异，像素值大于后一个像素值记为1，否则记为0
4. 根据像素间的灰度差异，生成一个二进制哈希码。
5. 对比二进制哈希码，计算汉明距离，距离越小，差别越小

与均值哈希相比，差值哈希在计算哈希码时更加注重像素间的相对关系，而不是简单地基于平均灰度值。这使得差值哈希在一定程度上能够更好地捕捉到图像的局部特征。

````pyth

#差值哈希
def dHash(img):
    img = cv2.resize(img,(8,9))
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    hashstr = ''
    for i in range(gray.shape[0]):
        for j in range(gray.shape[1]-1):
            if gray[i][j] > gray[i][j+1]:
                hashstr += 1
            else:
                hashstr += 0
    return hashstr
````

