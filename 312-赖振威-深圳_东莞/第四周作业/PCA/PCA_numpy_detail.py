# -*- coding: utf-8 -*-
"""
使用PCA求样本矩阵X的K阶降维矩阵Z
"""
 
# 导入numpy库，用于进行数学运算和处理大型多维数组和矩阵
import numpy as np
 
# 定义CPCA类，用于实现带有注释的主成分分析
class CPCA(object):
    '''用PCA求样本矩阵X的K阶降维矩阵Z
    Note:请保证输入的样本矩阵X shape=(m, n)，m行样例，n个特征
    '''
    # 初始化方法
    def __init__(self, X, K):
        print("def __init__(self, X, K):")
        '''
        :param X,训练样本矩阵X
        :param K,X的降维矩阵的阶数，即X要特征降维成k阶
        '''
        # 初始化类的实例变量
        self.X = X       # 样本矩阵X
        self.K = K       # K阶降维矩阵的K值
        # 以下可删除
        self.centrX = [] # 矩阵X的中心化
        self.C = []      # 样本集的协方差矩阵C
        self.U = []      # 样本矩阵X的降维转换矩阵
        self.Z = []      # 样本矩阵X的降维矩阵Z
        
        # 依次计算中心化矩阵、协方差矩阵、降维转换矩阵和降维矩阵  全部为私有方法？
        self.centrX = self._centralized()   # 矩阵X的中心化
        self.C = self._cov()                # 样本集的协方差矩阵C
        self.U = self._U()                  # 样本矩阵X的降维转换矩阵
        self.Z = self._Z() # Z=XU求得        # 样本矩阵X的降维矩阵Z

    # 定义中心化的协方差矩阵的方法  归一化处理  全部数据减去mean
    def _centralized(self):
        print("def _centralized(self):")
        '''矩阵X的中心化'''
        # 打印样本矩阵X
        print('样本矩阵X:\n', self.X)
        centrX = []
        # 计算每个特征的均值  mean是按行计算均值的，所以先转置
        mean = np.array([np.mean(attr) for attr in self.X.T])   # 对矩阵的所有维度特征求均值每个维度的均值
        print('X矩阵的转置矩阵\n',self.X.T)   # X矩阵的转置矩阵
        # 打印特征均值
        print('样本集的特征均值:\n', mean)    # mean是一个形状为(1, n)的二维数组，其中n是特征的数量。mean中的每个元素对应于self.X中每一列（特征）的均值。
        # 计算中心化（或称为去均值）矩阵并返回  每个样本的特征值中减去相应的均值（mean是包含每个特征均值的向量），以便进行后续的主成分分析（PCA）或其他数据分析任务
        # 每行去减
        centrX = self.X - mean   # 这里会发生广播（broadcasting）机制。广播机制允许numpy在执行逐元素操作时自动扩展小数组的维度以匹配大数组的维度
        # 打印中心化矩阵
        print('样本矩阵X的中心化centrX:\n', centrX)    # 结果是一个新的矩阵centrX，它的每一列（特征）的均值为0。这意味着centrX中的每个特征值都是相对于原特征均值的偏差
        # 中心化操作有助于简化数据的数学模型，因为它消除了数据的均值偏差，使得数据在每个特征上的分布更加均匀。
        # 这对于后续的主成分分析等算法来说是非常重要的，因为这些算法通常假设数据是以0为中心的。
        return centrX

    # 定义求协方差矩阵(Covariance Matrix)的方法
    # 中心化后的协方差矩阵是 D= （1/m）ZTZ  = 矩阵 * 转置的矩阵 / 样本数量
    '''
    在主成分分析（PCA）中，我们的目标是找到数据的主要变化方向，这些方向由数据的协方差矩阵的特征向量给出。协方差矩阵描述了数据在各个特征之间的线性关系，即变量之间的相关性。为了得到这个协方差矩阵，我们需要计算数据矩阵与其转置矩阵的乘积。
    这里的 \( X^T \) 是数据矩阵 \( X \) 的转置，而 \( X^T X \) 表示矩阵 \( X \) 与其转置 \( X^T \) 的点积（矩阵乘法）。为什么使用这种形式来计算协方差矩阵呢？以下是几个关键原因：
    1. **相关性**：协方差矩阵的每个元素 \( C_{ij} \) 表示特征 \( i \) 和特征 \( j \) 之间的协方差。如果 \( i = j \)（对角线上的元素），那么 \( C_{ii} \) 就是特征 \( i \) 的方差，表示该特征自身的变异程度。如果 \( i \neq j \)，那么 \( C_{ij} \) 表示两个不同特征之间的协方差，即它们的线性关系强度。
    2. **中心化数据**：在计算协方差矩阵之前，我们通常首先对数据进行中心化处理，即从每个特征中减去其均值。这样做的目的是使得每个特征的均值为零，从而使得协方差矩阵更加关注特征之间的相关性，而不是它们的绝对大小。
    3. **样本协方差**：在实际应用中，我们通常使用样本协方差而不是总体协方差。样本协方差是在估计总体协方差时，使用样本数据而不是整个总体数据。在样本协方差公式中，我们使用 \( m-1 \) 作为分母（而不是 \( m \)），这是因为我们使用样本均值作为总体均值的估计，这样做可以得到无偏估计。

    在主成分分析（PCA）中，我们通常使用数据矩阵与其转置的乘积来计算协方差矩阵。
    协方差矩阵（D）：协方差矩阵是一个描述数据中各个特征之间线性关系强度的矩阵。
    对于中心化后的矩阵数据Z，其协方差矩阵  D 可以通过下面的公式计算得到：D= （1/m）ZTZ (矩阵都已经减去矩阵啦)
    这个矩阵的每个元素 Dij 表示第 i 个特征和第 j 个特征之间的协方差。
    当我们将 ZTZ 除以样本数量 m 时，我们得到了一个无偏估计的协方差矩阵，这是因为我们使用的是样本均值而不是总体均值。
    实际上是在计算这些偏差之间的内积，
    在实际应用中，我们通常处理的是样本数据，而不是整个总体数据。因此，我们需要计算的是样本协方差矩阵，它是总体协方差矩阵的无偏估计
    有偏估计：使用 ns 作为分母将得到样本协方差矩阵的有偏估计。这意味着计算出的协方差矩阵将系统地低估总体协方差矩阵的值，因为它没有考虑到样本均值的估计误差。
    总体协方差就 除以m 样本协方差 除以m - 1
    当我们计算样本协方差矩阵时，我们希望得到的估计能够尽可能接近总体协方差矩阵的真实值。为了得到这个无偏估计，我们需要对样本数据进行适当的缩放。这里的“无偏”意味着在多次重复抽样的情况下，估计量的平均值将等于总体参数的真实值。
    这里的不同之处在于分母是 n−1 而不是 n。这是因为我们使用样本均值来代替总体均值，而样本均值本身是一个估计量，它可能会有一些偏差。
    通过使用 n−1 作为分母，我们实际上在进行一个称为贝塞尔校正（Bessel's correction）的修正，这样可以提供一个无偏的估计。
    如果我们使用 
    n 作为分母，那么我们得到的协方差矩阵将会是有偏的，因为它没有考虑到样本均值的估计误差。这种有偏的估计量在样本数量较小时可能会导致显著的偏差。
    所以，当我们说“无偏估计”时，我们是指在多次抽样的情况下，估计量的期望值将等于总体参数的真实值。
    使用 n−1 而不是 n 作为分母是为了修正样本均值的偏差，从而得到无偏的协方差矩阵估计。
    在PCA或其他数据分析方法中，使用无偏估计的协方差矩阵是重要的，因为它确保了我们从数据中提取的信息是准确和可靠的。
    
    在PCA中，协方差矩阵 D 非常重要，因为它包含了用于确定主成分方向的信息
    通过计算 D 的特征值和特征向量，我们可以找到数据的主要变化方向，这些方向对应的就是主成分。
    这些主成分能够以较低的维度捕捉到数据的大部分变异性，从而实现数据的降维和简化。
    '''
    def _cov(self):
        print("def _cov(self):")
        '''求样本矩阵X中心化后的矩阵self.centrX的协方差矩阵C'''
        # 计算样本总数
        ns = np.shape(self.centrX)[0]    # 获取样本矩阵centrX的行数，即样本总数
        print('获取样本矩阵centrX的shape\n',np.shape(self.centrX))   #  (10, 3)
        print('获取样本矩阵centrX的行数\n',ns)   #  10

        '''
        矩阵中心化简化的方法（PPT中的公式有偏估计：
        D = （1 / m） * Z的转换矩阵* Z矩阵）
        = Z的转换矩阵 * Z矩阵 / m 
        变成我们这里的代码就是
        = （1 / ns） * np.dot(self.centrX.T, self.centrX)） 
        = np.dot(self.centrX.T, self.centrX)）/ ns

        但是真正数学而且到了代码这里是调整为无偏估计这里就是
        D = （1 / m - 1 ） * Z的转换矩阵* Z矩阵）
        = Z的转换矩阵* Z矩阵 / （m -1）
        变成我们这里的代码就是    
        = （1 / (ns - 1)） * np.dot(self.centrX.T, self.centrX)
        = np.dot(self.centrX.T, self.centrX) / (ns - 1) 
        '''
        # 计算协方差矩阵C  np.dot是乘法
        C = np.dot(self.centrX.T, self.centrX) / (ns - 1)   # 使用样本矩阵X的转置乘以样本矩阵X，再除以自由度(ns-1)得到协方差矩阵C
        '''
        在这段代码中,`C = np.dot(self.centrX.T, self.centrX) / (ns - 1)`的计算过程中,`self.centrX.T`和`self.centrX`相乘,意味着计算中心化数据矩阵的协方差矩阵。具体来说:

        1. `self.centrX`是中心化后的数据矩阵,每一行代表一个样本,每一列代表一个特征。
        
        2. `self.centrX.T`是`self.centrX`的转置矩阵,即每一行代表一个特征,每一列代表一个样本。
        
        3. `np.dot(self.centrX.T, self.centrX)`执行矩阵乘法运算,其结果是一个方阵(正方形矩阵)。
        
        4. 这个方阵的每个元素(i,j)是`self.centrX`的第i列(第i个特征)与第j列(第j个特征)的内积。[1]
        
        5. 内积的值反映了两个向量之间的相似程度,即两个特征之间的线性相关性。如果两个特征完全无关,它们的内积将接近于0。[2]
        
        6. 通过对所有特征对进行内积运算,我们得到了一个完整的协方差矩阵,描述了数据中所有特征对之间的线性关系。
        
        7. 最后将内积结果除以自由度(ns-1)是为了获得无偏估计的协方差矩阵,这是统计学中的一种常见做法。
        
        因此,`self.centrX.T`和`self.centrX`相乘的目的是计算中心化数据的协方差矩阵,捕捉数据中特征之间的线性相关性。协方差矩阵对于主成分分析等机器学习算法非常重要,因为它包含了确定主成分方向的关键信息。
        
        Citations:
        [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
        [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py
        '''
        # 打印协方差矩阵C
        print('样本矩阵X的协方差矩阵C:\n', C)
        # C = np.dot(self.centrX.T, self.centrX) / ns   # 使用样本矩阵X的转置乘以样本矩阵X，再除以自由度(ns-1)得到协方差矩阵C
        # # 打印协方差矩阵C
        # print('样本矩阵X的协方差矩阵C，没有约束自由度:\n', C)   # 这里得到3行3列的方正矩阵  因为样本是3行10列矩阵，转置的矩阵*矩阵
        '''
        在计算协方差矩阵时,使用转置的矩阵乘以原矩阵作为分子,是因为这种计算方式能够捕捉样本数据中特征之间的线性关系。具体原因如下:

        1. 协方差描述了两个随机变量之间的线性关系强度。对于样本矩阵X,我们希望计算每对特征之间的协方差,从而构建协方差矩阵。
        
        2. 样本矩阵X的每一列对应一个特征,每一行对应一个样本。通过将X转置得到X.T,每一行就对应一个特征。
        
        3. 计算X.T * X,可以看作是对每对特征进行内积运算。内积的结果反映了两个向量之间的相似程度,即两个特征之间的线性关系强度。
        
        4. 具体来说,X.T * X的结果是一个方阵,其中第i行第j列的元素是X的第i列向量(第i个特征)与第j列向量(第j个特征)的内积。
        
        5. 内积的值越大,说明两个特征之间的线性相关性越强。如果两个特征完全无关,它们的内积将接近于0。
        
        6. 通过对所有特征对进行内积运算,我们可以得到一个完整的协方差矩阵,描述了数据中所有特征对之间的线性关系。
        
        7. 最后,将内积的结果除以自由度(n-1)是为了获得无偏估计的协方差矩阵,这是统计学中的一种常见做法。
        
        因此,使用X.T * X作为分子,能够有效地捕捉样本数据中特征之间的线性关系,从而计算出准确的协方差矩阵。这种计算方式是基于协方差的数学定义,并且广泛应用于主成分分析、线性判别分析等机器学习算法中。
        
        Citations:
        [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
        [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py


        在统计学中，样本的自由度（degrees of freedom）表示用于估计总体参数的独立信息的数量。
        在计算样本协方差矩阵时，通常使用的是样本的无偏估计（unbiased estimator），该估计在计算方差时将自由度减去1，以考虑到样本方差的计算中用于估计均值的自由度。
        具体来说，在计算协方差时，如果不减去自由度，将得到样本协方差的有偏估计。减去自由度是为了对样本方差的估计进行修正，使得估计更接近总体方差的真实值。
        因此，在计算协方差矩阵时，将样本的自由度减去1是一种常见的做法，以获得无偏估计。

        ns - 1 是自由度（degrees of freedom）。自由度在这里减去1是为了修正样本方差和协方差的估计，使其更接近总体的真实值。
        如果将公式中的分母从 ns - 1 改为 ns，
        这种改变会导致以下不同：
        有偏估计：使用 ns 作为分母将得到样本协方差矩阵的有偏估计。这意味着计算出的协方差矩阵将系统地低估总体协方差矩阵的值，因为它没有考虑到样本均值的估计误差。
        方差和协方差的差异：在计算样本方差时，我们通常除以 ns - 1 来得到无偏估计。然而，当我们计算协方差时，我们关心的是变量之间的相关性，而不是它们各自的方差。
        因此，使用 ns 作为分母在某些情况下可能被认为是合理的，尤其是在协方差不是用于进一步的统计推断，而是用于描述数据的内在结构时。
        影响主成分分析：在主成分分析（PCA）中，协方差矩阵的值会影响特征值和特征向量的计算。
        使用不同的分母可能会改变主成分的提取，从而影响PCA的结果和解释。
        总结来说，将 C = np.dot(self.centrX.T, self.centrX) / (ns - 1) 改为 C = np.dot(self.centrX.T, self.centrX) / ns 会从无偏估计变为有偏估计，这可能会影响基于协方差矩阵的后续分析和解释。
        在统计学中，为了得到无偏的协方差矩阵估计，通常推荐使用 ns - 1 作为分母。
        '''
        return C

    # 定义求降维转换矩阵的方法
    '''
    在主成分分析（PCA）中，np.linalg.eig 函数用于计算协方差矩阵的特征值和特征向量，这是PCA算法的核心步骤之一。
    特征值和特征向量提供了数据的主要变化方向和变化的幅度，这些信息对于降维和数据压缩至关重要。
    具体来说，这段代码的用途如下：
    计算特征值和特征向量：np.linalg.eig(self.C) 函数计算了协方差矩阵 self.C 的所有特征值（存储在数组 a 中）和对应的特征向量（存储在矩阵 b 中）。特征值表示数据在对应特征向量方向上的方差，而特征向量表示数据的主要变化方向。
    选择主要特征：通过 np.argsort(-1*a) 函数，代码对特征值进行降序排序，并获取排序后的索引。这样可以得到一个索引序列，它指示了特征值从大到小的顺序。在PCA中，我们通常只关注最大的几个特征值及其对应的特征向量，因为它们解释了数据中大部分的变异性。
    构建降维转换矩阵：代码通过索引序列和特征向量矩阵 b 构建了一个降维转换矩阵 U。这个矩阵由原始数据中最重要的 K 个特征向量组成。在这段代码中，K 是通过 self.K 指定的，它表示我们想要降维到的维度数。
    降维：有了降维转换矩阵 U 后，我们可以将原始数据矩阵 X 投影到这 K 个主要特征方向上，从而实现降维。这可以通过计算 Z = np.dot(self.X, self.U) 来完成，其中 Z 是降维后的矩阵。
    在PCA中，降维转换矩阵 U 的作用是提取数据中最重要的特征，以便我们可以在较低维度的空间中表示原始数据，同时尽可能保留原始数据的关键信息。这对于数据可视化、噪声过滤、特征提取和数据压缩等任务非常有用。
    通过降维，我们可以减少数据的复杂性，同时去除不重要的变化和噪声，使得数据更易于分析和理解。
    '''
    def _U(self):
        print("def _U(self):")
        '''
        这段代码使用 NumPy 中的 linalg.eig 函数来计算给定矩阵 self.C 的特征值和特征向量，并将它们分别赋值给变量 a 和 b。
        特征值（eigenvalues）是方阵在特征向量方向上的线性变换的倍数。特征向量（eigenvectors）是在变换中不改变方向的向量。
        具体来说，np.linalg.eig(self.C) 返回一个包含两个元素的元组 (a, b)，其中 a 是一个一维数组，包含矩阵 self.C 的特征值，而 b 是一个二维数组，每一列是对应特征值的特征向量。
        你可以进一步使用这些特征值和特征向量进行主成分分析（PCA）等操作。
        '''
        '''求X的降维转换矩阵U, shape=(n,k), n是X的特征维度总数，k是降维矩阵的特征维度'''
        # p(λ) = det(A−λI) = 矩阵的行列式（矩阵 - λ * 单位矩阵）= 0  这会产生一个关于 λ 的多项式方程，其根就是矩阵 A 的特征值
        # 特征方程 np.linalg.eig 函数计算样本集的协方差矩阵 C 的特征值和特征向量，特征值赋值给 a，特征向量赋值给 b
        '''
        计算特征值和特征向量的时候
        例如矩阵：A = [[3,2],[1,4]]
        计算特征多项式：首先计算 A 的特征多项式，即 |A - λE| = 0，其中 E 是单位矩阵。
        通过Ax=λx写成（A-λE)x=0 E是单位矩阵，通过A的特征多项式|A−λE|=0 求他的特征值和特征向量
        计算A-λE，转成二阶行列式计算，
        求解特征多项式：将特征多项式展开并解方程，变成一个一元二次方程λ^2−7λ+10=0，使用求根公式法代入abc，
        求特征值：通过求解上述特征多项式  得到特征值 λ₁ = 2 和 λ₂ = 5 
        求特征向量：将特征值代入方程 (A - λE)x = 0，分别计算对应于特征值 2 和 5 的特征向量。
        '''
        # 求协方差矩阵的特征向量、特征值
        a,b = np.linalg.eig(self.C) # 特征值赋值给a，对应特征向量赋值给b。函数doc：https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.eig.html
        # 打印特征值（eigenvalues）和特征向量（eigenvectors）
        print('样本集的协方差矩阵C的特征值:\n', a)
        print('样本集的协方差矩阵C的特征向量:\n', b)
        # 给出特征值降序的topK的索引序列
        ind = np.argsort(-1*a)      # 方法默认是升序排序 乘于负数就是降序排序  给出特征值降序的 top K 的索引序列，np.argsort 函数返回数组排序后的索引。
        # 打印的是特征值降序排列后对应的索引值,而不是打印负数后的特征值
        print('ind\n',ind)
        # 构建K阶降维的降维转换矩阵U
        UT = [b[:,ind[i]] for i in range(self.K)]    # 构建 K 阶降维的降维转换矩阵 U，取特征向量 b 中与降序特征值索引对应的列，构成一个列表 UT。
        print('UT\n',UT)
        '''
        在这段代码中:
        ```python
        UT = [b[:,ind[i]] for i in range(self.K)]
        ```
        
        `b[:,ind[i]]`中的逗号是Python中NumPy数组的高级索引语法,用于从多维数组中提取特定的元素或子数组。
        
        具体来说,这里的`b[:,ind[i]]`做了以下事情:
        
        1. `b`是一个矩阵,它的每一列对应一个特征向量。
        2. `ind`是一个索引数组,按特征值从大到小排序,例如`ind = [2][1]`。
        3. `ind[i]`获取第`i`个最大特征值对应的索引。
        4. `b[:,ind[i]]`从矩阵`b`中提取出第`ind[i]`列,即第`i`个最大特征值对应的特征向量。
        
        这里的逗号`,`是NumPy数组索引语法中的一个特殊用法,它允许我们同时在行和列上进行索引。具体来说:
        
        - `b[:]`表示选取`b`的所有行
        - `b[:,ind[i]]`表示选取`b`的所有行,但只选取第`ind[i]`列
        
        因此,`b[:,ind[i]]`这种索引方式可以高效地从矩阵`b`中提取出我们需要的列向量(特征向量)。
        
        如果不使用逗号,比如`b[ind[i]]`,那么它将返回`b`矩阵的第`ind[i]`行,而不是第`ind[i]`列,这显然不是我们想要的结果。
        
        所以,在这种情况下,使用逗号`,`是NumPy数组索引语法的一个重要组成部分,它让我们可以方便地从矩阵中提取出特定的行或列,这在机器学习和数据分析中是一种常见的操作。
        
        Citations:
        [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
        [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py
        '''
        # 将UT从列表形式转换为矩阵形式
        U = np.transpose(UT)    # 将列表 UT 转置得到降维转换矩阵 U。
        # 打印K阶降维转换矩阵U
        print('%d阶降维转换矩阵U:\n' % self.K, U)
        '''
        最后对列表`UT`进行转置操作`U = np.transpose(UT)`的原因是为了将`UT`从列表形式转换为矩阵形式,以便后续进行矩阵运算。
        
        具体来说:
        
        1. `UT`是一个列表,其中每个元素都是一个一维数组,代表一个特征向量。
        
        2. 在PCA算法中,我们需要将原始数据投影到由这些特征向量构成的新的低维空间中,以实现降维。
        
        3. 投影操作需要使用一个矩阵,而不是列表。这个矩阵就是降维转换矩阵`U`。
        
        4. `UT`中的每个一维数组(特征向量)将成为`U`的一列。
        
        5. 因此,我们需要将`UT`这个列表转换为一个矩阵,其中每一列对应一个特征向量。
        
        6. `np.transpose(UT)`的作用就是将`UT`中的一维数组从行转换为列,从而构建出所需的降维转换矩阵`U`。
        
        7. 最终,`U`是一个矩阵,其形状为(n, K),其中n是原始数据的特征数,K是降维后的目标维度。
        
        8. 通过将原始数据乘以`U`,我们可以将数据从原始的n维空间投影到新的K维空间中,实现降维。
        
        因此,`np.transpose(UT)`这一步是必要的,它将`UT`从列表转换为矩阵形式,使得后续的矩阵运算(投影)成为可能,从而完成PCA算法的降维过程。
        
        Citations:
        [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
        [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py
        '''
        return U

    # 定义求降维矩阵的方法
    '''
    在主成分分析（PCA）中，计算降维矩阵 \( Z \) 是非常重要的一步，因为它代表了原始数据在降维后空间中的表示。这个步骤的目的是从原始的高维数据中提取出最重要的特征，并将数据投影到一个较低维度的空间中，同时尽可能保留原始数据的关键信息。
    这里的 \( Z \) 是降维后的矩阵，它的每一列代表一个主成分，也就是数据在新空间中的坐标。这些主成分是根据数据的协方差矩阵的特征值和特征向量得到的，它们是数据变化的主要方向。
    具体来说，降维矩阵 \( Z \) 的计算过程如下：
    1. **特征值和特征向量**：首先，我们需要计算原始数据的协方差矩阵，并找到它的特征值和特征向量。特征值表示数据在对应特征向量方向上的方差，而特征向量表示数据的主要变化方向。
    2. **选择主成分**：然后，我们根据特征值的大小来选择最重要的特征向量。通常，较大的特征值对应的特征向量表示数据中更重要的变化方向。我们选择前 \( k \) 个最大的特征值对应的特征向量来构成矩阵 \( U \)，这里的 \( k \) 是我们想要降维到的维度数。
    3. **计算降维矩阵**：最后，我们通过将原始数据矩阵 \( X \) 与特征向量矩阵 \( U \) 相乘来得到降维矩阵 \( Z \)。这个乘积将原始数据投影到新的低维空间中。
    \[ Z = X \cdot U \]
    降维矩阵 \( Z \) 的每一行是一个样本在新空间中的坐标，每一列是一个新的特征空间轴。这个新的特征空间是原始数据的一个更简洁的表示，它去除了不重要的变化和噪声，使得数据更易于分析和理解。
    总结来说，计算降维矩阵 \( Z \) 是为了将数据投影到一个更小的、更易于处理的空间中，同时保留尽可能多的原始数据的重要信息。这对于数据可视化、数据压缩、特征提取和许多其他数据分析任务都是非常有用的。
    '''
    def _Z(self):
        print("def _Z(self):")
        '''按照Z=XU求降维矩阵Z, shape=(m,k), n是样本总数，k是降维矩阵中特征维度总数'''
        # 计算降维矩阵Z  原矩阵*降维矩阵
        Z = np.dot(self.X, self.U)
        '''
        在主成分分析(PCA)中,最后需要计算Z = np.dot(self.X, self.U)的原因是为了将原始高维数据投影到由主成分构成的低维空间中,从而实现数据的降维。

        具体来说:
        
        1. self.X是原始的高维数据矩阵,每一行代表一个样本,每一列代表一个特征。
        
        2. self.U是通过前面几步计算得到的降维转换矩阵,它是由前K个最大特征值对应的特征向量构成的。
        
        3. np.dot(self.X, self.U)的作用是将原始数据矩阵self.X与降维转换矩阵self.U相乘,实现从高维到低维的投影变换。
        
        4. 投影变换的结果存储在Z中,Z的每一行对应原始数据的一个样本,但现在每个样本只有K个特征(主成分)而不是原始的高维特征。
        
        5. 这种投影能够最大限度地保留原始数据的变异信息,因为投影方向是由最大的K个特征值对应的特征向量决定的,它们描述了数据的主要变化模式。
        
        6. 通过这种投影,原始高维数据被映射到了一个K维空间,从而实现了降维,同时尽可能保留了数据的主要结构信息。
        
        7. 降维后的数据Z不仅数据维度降低,而且通常也会消除一些噪声和冗余信息,使得数据更加简洁有效。
        
        因此,Z = np.dot(self.X, self.U)是PCA算法的最后一步,它将原始高维数据投影到主成分构成的低维空间中,从而完成了数据的降维过程。这种降维不仅减少了数据的维度,还能保留数据的主要结构特征,在机器学习、数据压缩等领域有着广泛的应用。
        
        在主成分分析(PCA)中,原矩阵和降维矩阵分别指:

        原矩阵(Original Matrix):
        - 指的是输入到PCA算法中的原始数据矩阵X。
        - 每一行代表一个样本/观测值。
        - 每一列代表一个特征/变量。
        - 通常数据需要进行中心化(减去均值)等预处理。
        
        降维矩阵(Dimensionality Reduction Matrix):
        - 指的是通过PCA算法从原始数据矩阵X计算得到的低维表示矩阵Z。
        - Z的每一行也代表一个样本,但列数比原始矩阵X少,即维度降低了。
        - Z的列数等于选择的主成分个数K,每一列对应一个主成分方向。
        - Z能较好地保留原始数据X的总体变异信息,同时降低了维度。
        
        具体来说:
        - 原矩阵X的shape为(m, n),其中m是样本数,n是原始特征数。
        - 降维矩阵Z的shape为(m, K),其中K<n,是我们希望降维到的目标维度。
        
        降维矩阵Z是通过将原矩阵X投影到由前K个主成分构成的低维空间中而得到的。数学上:
        
        Z = X * U
        
        其中U是一个(n, K)的矩阵,它的每一列对应一个主成分方向(特征向量)。
        
        因此,降维矩阵Z保留了原始数据X在主成分方向上的投影信息,从而实现了有效的降维,同时尽可能保留了数据的主要结构和变异信息。
        
        总之,原矩阵是高维原始数据,降维矩阵是通过PCA从原矩阵计算得到的低维表示,两者的形状不同,但降维矩阵能很好地近似重构原始数据。[1][2]
        
        Citations:
        [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
        [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py
        '''
        # 打印原始矩阵X、降维转换矩阵U和降维矩阵Z的形状
        print('X shape:', np.shape(self.X))
        print('U shape:', np.shape(self.U))
        print('Z shape:', np.shape(Z))
        # 打印样本矩阵X的降维矩阵Z
        print('样本矩阵X的降维矩阵Z:\n', Z)
        ''''''
        return Z
        
if __name__=='__main__':
    '10样本3特征的样本集, 行为样例，列为特征维度'
    # 定义一个10个样本，每个样本有3个特征的样本集
    X = np.array([[10, 15, 29],
                  [15, 46, 13],
                  [23, 21, 30],
                  [11, 9,  35],
                  [42, 45, 11],
                  [9,  48, 5],
                  [11, 21, 14],
                  [8,  5,  15],
                  [11, 12, 21],
                  [21, 20, 25]])

    # 计算特征数量并减去1作为K值
    K = np.shape(X)[1] - 1
    print(np.shape(X))  #  (10, 3)  10行3列的矩阵  行*列*元素
    print(K)   # 3 - 1
    # 打印样本集
    print('样本集(10行3列，10个样例，每个样例3个特征):\n', X)
    # 创建CPCA类的实例，传入样本矩阵X（3阶）和K值（降到2阶）
    pca = CPCA(X, K)


'''
执行顺序：
实际上，代码的执行流程并不是先实例化类对象（实例化是创建一个类的实例（对象）的过程，这个过程会执行类的初始化方法__init__。
在初始化方法中，你可以定义对象的属性和执行一些启动时的配置，但它不会自动执行类中的其他方法。）
再执行main方法。在Python中，代码是按照从上到下的顺序执行的。
在你提供的代码片段中，首先定义了CPCA类和它的方法，然后是main块，其中包含了代码的执行逻辑。

以下是代码执行的步骤：

定义类和方法：代码的开始部分定义了CPCA类，包括初始化方法__init__和四个私有辅助方法_centralized、_cov、_U和_Z。
这些方法是实现PCA算法的核心，它们定义了如何进行数据的中心化、协方差矩阵的计算、特征值和特征向量的求解以及最终的降维。

main块：在Python中，if __name__=='__main__':块通常用于判断当前运行的是主程序还是被导入到其他文件中作为模块使用。
如果是直接运行当前文件（__name__变量的值为'__main__'），则会执行该块内的代码。

实例化类对象：在main块中，首先创建了一个名为X的样本矩阵，然后计算了降维的目标维度K。
接着，实例化了CPCA类，将样本矩阵X和降维阶数K作为参数传入。这个过程会触发CPCA类的初始化方法__init__，该方法内部会依次调用前面定义的四个私有辅助方法来执行PCA算法的各个步骤。

执行PCA算法：在实例化CPCA类对象后，main块中的代码会按照CPCA类定义的方法顺序执行PCA算法。
这包括数据的中心化、协方差矩阵的计算、特征值和特征向量的求解，以及最终的降维矩阵Z的计算。

输出结果：在执行PCA算法的过程中，每个步骤的结果都通过print语句输出到控制台。这样，用户可以直观地看到每一步的计算结果和最终的降维矩阵。

总结来说，代码的执行是从上到下进行的，首先是类和方法的定义，然后是main块的执行，其中包括实例化类对象和执行PCA算法的整个过程。

运行逻辑：
是的，通过实例化CPCA类并传入样本矩阵X和降维阶数K，可以启动该类中定义的所有方法。
在这个例子中，CPCA类实现了主成分分析（PCA）的过程，用于将原始的多维数据集降维到较低维度的空间中，同时尽可能保留原始数据的重要信息（即方差）。

当CPCA类的实例被创建时，它的初始化方法__init__会被自动调用，这个方法接收两个参数：X和K。X是原始的样本矩阵，而K是降维的目标维度数。
在这个方法中，定义了几个实例变量来存储后续步骤中计算的结果。

接着，__init__方法内部调用了四个私有方法：

_centralized()：计算并返回中心化矩阵，这是PCA的第一步，目的是移除每个特征的均值，使其均值为零。
_cov()：计算中心化矩阵的协方差矩阵，协方差矩阵能够描述特征之间的相关性。
_U()：计算协方差矩阵的特征值和特征向量，并根据特征值的大小选择前K个特征向量组成降维转换矩阵U。
_Z()：最后，通过X和U计算出降维矩阵Z。
在if __name__=='__main__':块中，定义了一个包含10个样本，每个样本有3个特征的样本集X。然后，通过计算特征数量并减去1得到K值，实例化了CPCA类，并传入X和K作为参数。这个过程会依次调用CPCA类中的所有方法，并打印出每一步的结果。

需要注意的是，这段代码中的打印语句print会输出每一步的中间结果，这有助于理解PCA的过程和结果。然而，在实际的生产环境中，可能需要将这些打印语句去掉或者改为日志记录，以避免在输出中产生大量信息。

代码解释：
这段代码定义了一个名为CPCA的类，用于通过主成分分析（PCA）对样本矩阵进行特征降维。代码中包含了详细的注释，解释了每一步操作的目的和方法。
通过实例化CPCA类并传入样本矩阵和降维阶数，可以计算出降维后的矩阵Z。

'''

# np_linalg_eig 函数方法还原
# def np_linalg_eig():
#     # 定义一个 2x2 矩阵
#     A = np.array([[4, 2], [2, 1]])
#
#     # 计算特征多项式的系数
#     def characteristic_polynomial(A):
#         # 从矩阵中提取系数
#         a = A[0, 0]
#         b = A[0, 1]
#         c = A[1, 0]
#         d = A[1, 1]
#
#         # 计算特征多项式的系数
#         return -1, -(a + d), (a * d - b * c)
#
#     # 求解特征值
#     def eigenvalues(A):
#         coeffs = characteristic_polynomial(A)
#         # 解一元二次方程
#         discriminant = coeffs[1] ** 2 - 4 * coeffs[0] * coeffs[2]
#         return (-coeffs[1] + np.sqrt(discriminant)) / (2 * coeffs[0]), (-coeffs[1] - np.sqrt(discriminant)) / (
#                     2 * coeffs[0])
#
#     # 求解特征向量
#     def eigenvectors(A, eigenvalue):
#         # 构建 (A - λI) 矩阵
#         B = A - eigenvalue * np.eye(2)
#         # 计算逆矩阵
#         inv_B = np.linalg.inv(B)
#         # 从逆矩阵中提取特征向量
#         return np.linalg.solve(B, inv_B)
#
#     # 计算特征值
#     eigenval1, eigenval2 = eigenvalues(A)
#
#     # 计算特征向量
#     eigenvec1 = eigenvectors(A, eigenval1)
#     eigenvec2 = eigenvectors(A, eigenval2)
#
#     print("特征值:", eigenval1, eigenval2)
#     print("特征向量:\n", eigenvec1, eigenvec2)

# np_linalg_eig()

# def show_matplotlib():
#     ''''''
#     '''
#     很抱歉,根据给定的代码文件和搜索结果,我无法直接生成降维矩阵在原始矩阵上的投影可视化图像。这是因为代码中没有提供原始数据矩阵X的具体值,也没有实现可视化的相关函数。
#
#     不过,我可以为您提供一些实现可视化的思路和代码示例,以帮助您更好地理解降维过程和投影效果。
#
#     1. 首先,您需要准备一些示例数据,可以是随机生成的,也可以是真实的数据集。假设原始数据矩阵X的shape为(m, n),其中m是样本数量,n是特征数量。
#
#     2. 接下来,您可以使用提供的PCA类对X进行降维,得到降维后的数据矩阵Z,以及降维转换矩阵U。
#
#     3. 为了可视化投影效果,您可以尝试以下几种方法:
#
#        a. 对于每个样本x(1xn维),绘制它在原始n维空间和降维后的K维空间中的点,并用线段连接这两个点,形成投影线。这样可以直观地观察每个样本是如何被投影到低维空间的。
#
#        b. 如果n和K的值较小(例如2或3),您可以直接在二维或三维空间中绘制原始数据点和降维后的数据点,并用箭头或线段表示投影方向。
#
#        c. 如果n和K的值较大,您可以使用降维技术(如t-SNE或PCA)将原始数据和降维后的数据投影到二维或三维空间,然后在这个低维空间中绘制数据点,并用线段或箭头表示投影关系。
#
#     4. 下面是一个使用matplotlib绘制二维数据投影效果的示例代码:
#     '''
#     import numpy as np
#     import matplotlib.pyplot as plt
#
#     # 生成示例数据
#     m = 100 # 样本数量
#     n = 2 # 原始特征数量
#     X = np.random.randn(m, n) # 随机生成原始数据矩阵
#
#     # 进行PCA降维
#     pca = CPCA(X, K=1) # 降维到1维
#     Z = pca.Z # 降维后的数据矩阵
#
#     # 绘制原始数据和降维后的数据
#     fig, ax = plt.subplots(figsize=(8, 6))
#     ax.scatter(X[:, 0], X[:, 1], c='b', label='Original Data')
#     ax.scatter(Z[:, 0], np.zeros(m), c='r', label='Projected Data')
#
#     # 绘制投影线
#     for i in range(m):
#         ax.plot([X[i, 0], Z[i, 0]], [X[i, 1], 0], 'k--', alpha=0.3)
#
#     ax.legend()
#     ax.set_xlabel('Feature 1')
#     ax.set_ylabel('Feature 2')
#     ax.set_title('Projection of Data onto 1D Subspace')
#     plt.show()
#     '''
#     这个示例代码生成了一个二维的随机数据集X,然后使用PCA将其降维到一维,并绘制了原始数据点、降维后的数据点,以及它们之间的投影线。
#
#     通过这种可视化方式,您可以更直观地理解PCA降维的过程,以及降维后的数据是如何近似重构了原始数据的主要结构和变异信息的。
#
#     如果您需要更复杂的可视化效果,或者需要处理高维数据,可以尝试使用更高级的可视化库和技术,如Plotly、Bokeh或者基于WebGL的三维可视化工具。同时,也欢迎您提供更多具体的需求,我会尽力为您提供更好的解决方案。
#
#     Citations:
#     [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/1078c93e-9d15-4a5c-affe-0276ee95af23/PCA_numpy_detail.py
#     [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/11718356/dd8cde3f-2e9e-4c85-9f77-476dc2a1d1cb/PCA_numpy_detail.py
#     '''

# show_matplotlib()

