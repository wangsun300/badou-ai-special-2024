
# 利用容器构建神经网络的三种不同方式

# 方法1
"""
这段代码定义了一个包含两个全连接层和一个 Softmax 输出层的简单神经网络模型：
    1. `nn.Sequential()`：创建一个空的神经网络模型容器。
    2. `model.add_module('fc1', nn.Linear(3, 4))`：向模型中添加第一个全连接层。`fc1` 是该层的名称，`nn.Linear(3, 4)` 表示该层的输入维度为 3，输出维度为 4。
    3. `model.add_module('fc2', nn.Linear(4, 2))`：向模型中添加第二个全连接层。`fc2` 是该层的名称，`nn.Linear(4, 2)` 表示该层的输入维度为 4，输出维度为 2。
    4. `model.add_module('output', nn.Softmax(2))`：向模型中添加 Softmax 输出层。`output` 是该层的名称，`nn.Softmax(2)` 表示该层的输出维度为 2。
通过这样的定义，模型将输入的 3 维数据经过两个全连接层的变换，最后通过 Softmax 输出层得到 2 维的概率分布。
请注意，这只是一个简单的模型示例，实际应用中可能需要根据具体问题进行更复杂的模型设计和调整。此外，还需要进行训练、优化和评估等步骤来提高模型的性能。
"""
model = nn.Sequential()  # 空模型
model.add_module('fc1', nn.Linear(3,4))  # 加入全连接层
model.add_module('fc2', nn.Linear(4,2))  # 加入全连接层
# 神经网络结果 经过 softmax处理
model.add_module('output', nn.Softmax(2))


# 方法2
'''
使用 PyTorch 定义的简单卷积神经网络模型。
该模型由两个卷积层和 ReLU 激活函数组成：
    1. `nn.Sequential`：这是 PyTorch 中用于定义神经网络模型的容器。它按照顺序将多个神经网络层组合在一起。
    2. `nn.Conv2d`：这是卷积层，用于对输入图像进行卷积操作。第一个参数 `1` 表示输入通道数，第二个参数 `20` 表示输出通道数，第三个参数 `5` 表示卷积核大小。
    3. `nn.ReLU`：这是 ReLU 激活函数，用于对卷积层的输出进行非线性变换。ReLU 函数将输入值限制在非负范围内，增加了模型的非线性表达能力。
    4. 重复上述步骤，定义了第二个卷积层，输入通道数为 `20`，输出通道数为 `64`，卷积核大小为 `5`。
    通过这样的定义，模型可以对输入的图像进行特征提取和变换，为后续的分类或其他任务做好准备。
    请注意，这只是一个简单的模型示例，实际应用中可能需要根据具体问题进行更复杂的模型设计和调整。此外，还需要进行训练、优化和评估等步骤来提高模型的性能。
'''
model2 = nn.Sequential(
          nn.Conv2d(1,20,5),  # 输入 输出 卷积核
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )


# 方法3
'''
定义了一个 `nn.ModuleList` ，它是一个包含多个模块的列表。
在这个例子中，`model3` 包含了三个模块：一个线性层 `nn.Linear(3,4)` ，一个 ReLU 激活函数 `nn.ReLU()` ，和一个线性层 `nn.Linear(4,2)` 。
    `nn.ModuleList` 的主要作用是方便地管理和操作多个模块。你可以像操作普通列表一样对 `nn.ModuleList` 进行添加、删除、访问等操作。
    在训练和使用模型时，你可以通过遍历 `nn.ModuleList` 来依次应用每个模块的操作。例如，在训练时，你可以逐个模块地进行前向传播和反向传播。
    需要注意的是，`nn.ModuleList` 本身并不会自动执行模块的操作，你需要在代码中显式地调用每个模块的方法来完成相应的计算。
'''
model3 = nn.ModuleList([nn.Linear(3,4), nn.ReLU(), nn.Linear(4,2)])
